{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716be055",
   "metadata": {},
   "source": [
    "## BERTopic with \"LDA setting\"\n",
    "Source: https://maartengr.github.io/BERTopic/index.html#installation\n",
    "\n",
    "------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c62d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efecc",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"orban_speeches_en_thesis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the empty cells and convert column to list\n",
    "\n",
    "docs = data['speech'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the empty cells and convert column to list\n",
    "\n",
    "docs_tokenized = data['tokenized_speech'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f2ce3",
   "metadata": {},
   "source": [
    "### Split docs to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if needed\n",
    "\n",
    "#docs_sentences = []\n",
    "\n",
    "#for doc in docs:\n",
    "    #docs_sentences.append(doc.split(\". \"))\n",
    "    \n",
    "#print(docs_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a9185",
   "metadata": {},
   "source": [
    "### Preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfe3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make a list of the most common n words\n",
    "## if n parameter isn't specified by the user, it returns all of the words from the list\n",
    "\n",
    "def freq_topn(str_list, n = None):\n",
    "    frequency = Counter(str_list).most_common(n)\n",
    "    freq_topn_list = []\n",
    "    \n",
    "    for tupl in frequency:\n",
    "        freq_topn_list.append(tupl[0])\n",
    "        \n",
    "    return freq_topn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find tokens ending in \".hu\" and make a list of them\n",
    "    \n",
    "def get_hu(str_list):\n",
    "    hu_set = {x for x in str_list if re.search(r'\\.hu$', x)}\n",
    "    hu_list = list(hu_set)\n",
    "    return hu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove specified stopwords\n",
    "\n",
    "def preprocess_data(documents, stop_words):\n",
    " \n",
    "    # Tokenize and remove stopwords\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in documents]\n",
    " \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be0d4a",
   "metadata": {},
   "source": [
    "### Preprocess: dicover custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80237666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the speeches into one string\n",
    "\n",
    "speeches_longstring = ' '.join(map(str, docs_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996298cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove default stopwords to find other frequent words beyond them\n",
    "\n",
    "stop_words_default = stopwords.words(\"english\")\n",
    "tokenize_words = speeches_longstring.split()\n",
    "\n",
    "filtered_speeches_lst = [w for w in tokenize_words if not w in stop_words_default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom stopwords\n",
    "\n",
    "stop_words_custom = stopwords.words(\"english\") + freq_topn(filtered_speeches_lst, 95) + get_hu(filtered_speeches_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af6353",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73c044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-calculate embeddings --> feed them to BERTopic to skip calculating embeddings each time\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4136b",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "It is important to include all the words when embedding! The methods below will help in the model to remove them later when creating the topic representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=stop_words_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be5268",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db76073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning UMAP -- random state\n",
    "\n",
    "umap_model = UMAP(\n",
    "    random_state=42 #for the sake of reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a63f0",
   "metadata": {},
   "source": [
    "### Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2da588",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    # pipeline models\n",
    "    umap_model=umap_model,\n",
    "    \n",
    "    # remove stop words\n",
    "    vectorizer_model=vectorizer_model,\n",
    "\n",
    "    #hyperparameters\n",
    "    top_n_words = 25, # number of returned topic words\n",
    "    nr_topics = 10, # number of topics to return\n",
    "    low_memory = False, # set to True if yor computer needs it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs_tokenized, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4449b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return generated topics\n",
    "\n",
    "#topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the topic words, outlier topic excluded (list of list of str)\n",
    "\n",
    "topic_list = topic_model.get_topic_info()[\"Representation\"].to_list()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccb4a8",
   "metadata": {},
   "source": [
    "### Saving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e737ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics=topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ed929",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics_df = pd.DataFrame()\n",
    "\n",
    "# Iterates throught the dictionary and creates a DataFrame\n",
    "for key, values in all_topics.items():\n",
    "    words = [pair[0] for pair in values]\n",
    "    probs = [pair[1] for pair in values]\n",
    "    all_topics_df[f\"{key}_word\"] = words\n",
    "    all_topics_df[f\"{key}_prob\"] = probs\n",
    "\n",
    "#print(all_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df\n",
    "all_topics_df.to_excel(\"Topic_outputs/Final/BERT_LDA_25words.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bb616",
   "metadata": {},
   "source": [
    "### Coherence score\n",
    "**<span style=\"color: crimson\"> Important! For this part, top_n_words must set to 10 in the model!</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = preprocess_data(docs_tokenized, stop_words_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary (the dictionary is a mapping between words and their integer IDs)\n",
    "id2word = corpora.Dictionary(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model using the coherence score\n",
    "coherence_model = CoherenceModel(topics=topic_list, \n",
    "                                 texts=processed_texts, \n",
    "                                 dictionary=id2word, \n",
    "                                 coherence=\"c_npmi\")\n",
    "\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(\"Coherence Score: \", coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9cc77e",
   "metadata": {},
   "source": [
    "### Topic diversity\n",
    "**<span style=\"color: crimson\"> Important! For this part, top_n_words must set to 25 in the model!</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60200ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list_all = [word for topic in topic_list for word in topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic diversity = ratio of unique words in the top 25 words of topics\n",
    "# by converting a list to set, it removes the duplicates\n",
    "\n",
    "topic_diversity = len(set(topic_list_all))/len(topic_list_all)\n",
    "print(topic_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0862a",
   "metadata": {},
   "source": [
    "### Set topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb05cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.set_topic_labels(\n",
    "    {0: \"Sikeres fejlődés\", \n",
    "     1: \"Európa\", \n",
    "     2: \"Orosz-ukrán háború\",\n",
    "     3: \"Hazafiság\",\n",
    "     4: \"Covid\",\n",
    "     5: \"Egyház\",\n",
    "     6: \"Sport\",\n",
    "     7: \"Kína\",\n",
    "     8: \"Ipar\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a64e7",
   "metadata": {},
   "source": [
    "### Topic visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997176b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics(width=600, height=600, title=\"\", custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804640e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(custom_labels=True, width=550, height = 450, title=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(\n",
    "    docs_tokenized, \n",
    "    embeddings=embeddings, \n",
    "    #hide_annotations=True, \n",
    "    width=800, \n",
    "    height=800, \n",
    "    title=\"\",\n",
    "    custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(custom_labels=True, width= 700, title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055a89c",
   "metadata": {},
   "source": [
    "### Saving model (serialization)\n",
    "https://maartengr.github.io/BERTopic/getting_started/serialization/serialization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"Modellek/bertopic_lda\", serialization=\"pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
